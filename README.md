# Fredholm Neural Networks
This repository contains the codes in Python and MATLAB that were developed and used for the **Fredholm Neural Network (FNN)** and **Potential Fredholm Neural Network (PFNN)** framework.

The theoretical framework, used for both the forward and inverse problems, is briefly described below. For the full details see the papers:

1. Fredholm Neural Networks - https://epubs.siam.org/doi/full/10.1137/24M1686991?casa_token=LUOO2mbMhAcAAAAA%3AQUFO1UaeNBfHdXzGBU2c_oZFy2vwIea8jtON46KL_TC_wkjEke7VEW-lLoQ9bY0Gw9BZcFy1
2. Fredholm Neural Networks for forward and inverse problems in elliptic PDEs - https://arxiv.org/abs/2507.06038

To reference this code please cite:

@article{georgiou2025fredholm,
  title={Fredholm neural networks},
  author={Georgiou, Kyriakos and Siettos, Constantinos and Yannacopoulos, Athanasios N},
  journal={SIAM Journal on Scientific Computing},
  volume={47},
  number={4},
  pages={C1006--C1031},
  year={2025},
  publisher={SIAM}
}

and/or 

@article{georgiou2025fredholm,
  title={Fredholm Neural Networks for forward and inverse problems in elliptic PDEs},
  author={Georgiou, Kyriakos and Siettos, Constantinos and Yannacopoulos, Athanasios N},
  journal={arXiv preprint arXiv:2507.06038},
  year={2025}
}

# 1.  Fredholm Neural Networks for Integral Equations

## Background
The basis of FNNs is the method of successive approximations (fixed point iterations) to approximate the fixed-point solution to Fredholm Integral Equations (FIEs). Specifically, the framework is built upon linear FIEs of the second kind, which are of the form:

$$f(x) = g(x) + \int_{\Omega}K(x,z) f(z)dz, $$

as well as the non-linear counterpart,

$$f(x) = g(x) + \int_{\Omega}K(x,z) G(f(z))dz,$$

for some function $G: \mathbb{R} \rightarrow\mathbb{R}$ considered to be a Lipschitz function. 

We consider the cases where the integral operators are either contractive or non-expansive. This allows linear FIE defined by a non-expansive operator $\mathcal{T}$, and a sequence $\{\kappa_n\}, \kappa_n \in (0,1]$ such that $\sum_n \kappa_n(1-\kappa_n) = \infty$. Then, the iterative scheme:

$$f_{n+1}(x) = f_n(x) + \kappa_n(\mathcal{T}f_n(x) -f_n(x)) = (1-\kappa_n)f_n(x) + \kappa_n \mathcal{T} f_n(x),$$

with $f_0(x) = g(x)$, converges to the fixed point solution of the FIE, $f^{*}(x)$.

When $\mathcal{T}$ is a contraction, we can obtain the iterative process:
$$f_n(x)= g(x) +  \int_{\Omega}f_{n-1})(x), \,\,\ n \geq 1,$$
which converges to the fixed point solution. This is often referred to as the method of successive approximations.

## FNN construction for forward FIEs 

Fredholm Neural Networks are based on the observation that the FIE approximation $f_K(x)$ can be implemented as a deep neural network with a one-dimensional input $x$, $M$ hidden layers, a linear activation function and a single output node corresponding to the estimated solution $f(x)$. The weights and biases are:

$$
W_1 =
\begin{bmatrix}
\kappa g(z_1) \\
\vdots \\
\kappa g(z_{N})
\end{bmatrix},
\qquad
b_1 =
\begin{bmatrix}
0 \\
\vdots \\
0
\end{bmatrix}.
$$

for the first hidden layer,

$$
W_m =
\begin{bmatrix}
K_D(z_1) & K(z_1,z_2)\,\Delta z & \cdots & K(z_1,z_N)\,\Delta z \\
K(z_2,z_1)\,\Delta z & K_D(z_2) & \cdots & K(z_2,z_N)\,\Delta z \\
\vdots & \vdots & \ddots & \vdots \\
K(z_N,z_1)\,\Delta z & K(z_N,z_2)\,\Delta z & \cdots & K_D(z_N)
\end{bmatrix},
$$

and

$$
b_m =
\begin{bmatrix}
\kappa g(z_1) \\
\vdots \\
\kappa g(z_N)
\end{bmatrix},
\qquad m=2,\dots,M-1,
$$

where $K_D(z) := K(z,z)\,\Delta z + (1-\kappa_m)$. Finally,

$$
W_M =
\begin{bmatrix}
K(z_1,x)\,\Delta z \\
\vdots \\
K(z_{i-1},x)\,\Delta z \\
K_D(x) \\
K(z_{i+1},x)\,\Delta z \\
\vdots \\
K(z_N,x)\,\Delta z
\end{bmatrix},
\qquad
b_M = \kappa g(x),
$$

assuming $z_i = x$.


<img width="324" height="290" alt="Screenshot 2025-10-08 at 11 45 05 AM" src="https://github.com/user-attachments/assets/2cdfd98b-7c52-4119-999d-b1bc40732a6b" /> 
<img width="575" height="248" alt="Screenshot 2025-10-08 at 11 45 33 AM" src="https://github.com/user-attachments/assets/bbda1e93-36b5-4c83-afa3-8b86d9459996" />  

*Figure 1: Architecture of the Fredholm Neural Network (FNN). Outputs can be considered across the entire (or a subset of the) input grid, or for an arbitrary output vector as shown in the second graph, by applying the integral mapping one last time.*

Examples in Python can be seen [`here`](Scripts_and_Examples_Py/Fredholm_Integral_Equation_Examples.ipynb) and in matlab [`here`](Scripts_and_Examples/Fredholm_Integral_Equation_forward.m).

The corresponding classes are [`here`](Classes_Py/fredholm_nn_models.py) and [`here`](Classes/FredholmNeuralNetwork.m).



## Application to non-linear FIEs 

We can create an iterative process that "linearizes" the integral equation and allows us to solve a linear FIE at each step. To this end, consider the non-linear, non-expansive integral operator:

$$(\mathcal{T}f)(x) := g(x) + \int_{\Omega}K(x,z) G(f(z))dz.$$

Then, the iterative scheme $f_n(x) = \tilde{f}_n(x)$, where $\tilde{f}_n(x)$ is the solution to the linear FIE:

$$\tilde{f}_{n}(x) = ({L}\tilde{f}_{n-1})(x) + \int_{\Omega}K(x,z) \tilde{f}_{n}(z))dz,$$
    
where: 

$$(\mathcal{L}\tilde{f}_{n-1})(x) := g(x) + \int_{\Omega} K(x,y)\big( G(\tilde{f}_{n-1}(y)) - \tilde{f}_{n-1}(y)\big)dy,$$ 

for $n \geq 1$, converges to the fixed point $f^*$  which is a solution of the non-linear FIE.

<img width="636" height="207" alt="Screenshot 2025-10-08 at 1 35 31 PM" src="https://github.com/user-attachments/assets/f692d52e-21a0-4f2a-b668-f8a938527a3f" />

*Figure 2: Iterative process to solve the non-linear FIE using the Fredholm NN architecture.*

Examples in Python are in the corresponding section of the [`forward FIE notebook`](Scripts_and_Examples_Py/Fredholm_Integral_Equation_Examples.ipynb).


## Application to BVP ODEs

Consider a BVP of the form:

$$y''(x) + g(x)y(x) = h(x), 0<x<1,$$ 
    
with $y(0) = \alpha, y(1) = \beta$. Then we can solve the BVP by obtaining the following FIE:

$$u(x) = f(x) + \int_{0}^{1} K(x,t) u(t)dt,$$

where $u(x) = y''(x), f(x) = h(x) - \alpha g(x) - (\beta - \alpha) x g(x)$, and the kernel is given by:

$$ K(x,t) = 
    \begin{cases}
        t(1-x)g(x), \,\,\, 0 \leq t \leq x \\
        x(1-t)g(x), \,\,\, x\leq t \leq 1.
    \end{cases}$$
    
Finally, by definition of $u(x)$, we can obtain the solution to the BVP by:

$$y(x) = \frac{h(x) - u(x)}{g(x)}.$$

Examples in Python are in the corresponding section of the [`forward FIE notebook`](Scripts_and_Examples_Py/Fredholm_Integral_Equation_Examples.ipynb).

## Application to the inverse kernel problem (for FIEs)

This consists of taking as data two functions $f, g : \Omega \to \mathbb{R}$ and modeling an unknown kernel $K : \Omega \times \Omega \to \mathbb{R}$ (e.g., a neural network) so that $f$ satisfies a target integral equation. Hence, the inverse problem is: given $\tilde{f}$ and $g$, find $K$ such that the induced integral operator admits a solution $f$ that matches $\tilde{f}$ on the chosen collocation points. 

Our strategy uses the structure/convergence of the Fredholm NN: select parameters $\theta$ so that, when constructing the estimated kernel $K_\theta$ and feeding it into the Fredholm NN with M hidden layers, the network output $\hat{f}(x;\hat K_\theta)$ is close to the data $\tilde{f}$ under an appropriate loss.

We use two terms:

$$R_{\theta} = \sum_j w_j^2 $$

$${R}_{{FIE}}(\theta)= \frac{1}{N}\sum_{i=1}^{N}
\Big(\tilde{f}(x_i) - (\mathcal{T}_{\theta}\tilde{f})(x_i)\Big)^2 $$

with

$$ (\mathcal{T}_{\theta}\tilde{f})(x) := g(x) + \int_{\Omega} \hat{K}_{\theta}(x,y)\,\tilde{f}(y)\,dy.$$

The complete loss is

$$
L(\theta) = \frac{1}{N}\sum_{i=1}^{N} \Big(f(x_i) - \hat{f}(x_i;\hat{K}_{\theta})\Big)^2 + \lambda_{reg}\,{R}(\theta).
$$

Here, $\hat{f}(x;\hat{K}_{\theta})$ denotes the output of the Fredholm NN.


<img width="613" height="234" alt="Screenshot 2025-10-08 at 2 26 57 PM" src="https://github.com/user-attachments/assets/5bd73f8c-0b5a-4500-bafc-7535dfb46edc" />

*Figure 3: Algorithm to solve the inverse problem using the Fredholm NN framework.*

The implementation is given for a specific example in MATLAB, using the Levenberg-Marquardt training algorithm [`here`](Scripts_and_Examples/Fredholm_Integral_Equation_inverse.m).

# Potential Fredholm Neural Networks for elliptic PDEs

Here we briefly provide the background in Potential Theory and how it is applied in the context of FNNs, resulting in the Potential Frendholm Neural Network (PFNN), used to solve elliptic PDEs.

Consider the two-dimensional linear Poisson equation for $u(x)$:

$$\begin{cases}\Delta u(x) = \psi(x), & \text { for } x \in \Omega \\
u(x)= f(x) & \text { for } {x} \in \partial \Omega. \end{cases}$$

Its solution can be written via the double layer boundary integral given by:

$$u(x) =  \int_{\partial \Omega} \beta(y) \frac{\partial \Phi}{\partial n_{y}}(x, y) d \sigma_{y} + \int_{\Omega} \Phi(x,y) \psi(y) d y ,  x \in \Omega,$$

where $\Phi(x,y)$ is the fundamental solution of the Laplace equation, $n_y$ is the outward pointing normal vector to $y$, $\sigma_y$ is the surface element at point $y\in \partial \Omega$, and $\frac{\partial \Phi}{\partial n_{y}} = n_y \cdot \nabla_{ y}{\Phi}$. It can be shown that the following limit holds, as we approach the boundary: 

$$\lim _{\substack{x \rightarrow x^{\star} \\ x \in \Omega}}   \int_{\partial \Omega} \beta(y) \frac{\partial \Phi}{\partial n_y}(x, y) d \sigma_{y} =u\left({x}^{\star}\right) - \frac{1}{2} \beta(x^{\star}), \quad x^{\star} \in \partial \Omega.$$

Hence, the function $\beta({x}^{\star})$, defined on the boundary, must satisfy the Boundary Integral Equation (BIE):

$$\beta({x}^{\star}) = 2 \Big(f(x^{\star}) - \int_{\Omega} \Phi(x^*,y) \psi(y) dy \Big) - 2 \int_{\partial \Omega} \beta(y) \frac{\partial \Phi}{\partial n_{y}}(x^{\star}, y) d \sigma_{y},  x^{\star} \in \partial \Omega.$$

<img width="548" height="376" alt="Screenshot 2025-10-08 at 4 58 58 PM" src="https://github.com/user-attachments/assets/f9edb609-f257-4c06-b96e-7ee4095c34bd" />

*Figure 4: PFNN construction. The first component is a Fredholm Neural Network and the second encapsulates the representation of the double layer potential, decomposed into a the final hidden layer.*


### Poisson PDE - PFNN Construction 
The Poisson PDE 

$$
\begin{cases}
 \Delta u(x)  = \psi(x), \quad x \in \Omega \\ 
u(x) = f(x), \quad x \in \partial \Omega.   
\end{cases}
$$

can be solved using a Fredholm NN, with M+1 hidden layers, where the weights and biases of the M hidden layers are used iteratively solve the BIE on a discretized grid of the boundary, $y_1, \dots, y_N$, 
for which the final and output weights $W_{M+1} \in \mathbb{R}^{N \times N}, W_O \in \mathbb{R}^N$ are given by:

$$
W_{M+1}= I_{N \times N}, 
W_{O}= \left(\begin{array}{cccc}
\mathcal{D} \Phi(x, y_1)\Delta \sigma_y, & \mathcal{D} \Phi(x, y_2)\Delta\sigma_y, & \dots, & \mathcal{D} \Phi(x, y_N) \Delta \sigma_y
\end{array}\right)^{\top},
$$

where we define the simple operator $\mathcal{D} \Phi({x}, {y}):= \Big(\frac{\partial \Phi}{\partial n_y}(x, y)- \frac{\partial \Phi}{\partial n_y}(x^{\star}, y)\Big)$. The corresponding biases $b_{M+1} \in \mathbb{R}^{N}$ and $b_O \in \mathbb{R}$ are given by:

$$ b_{M+1} = \left(\begin{array}{ccc}
-\beta(x^{\star}), \dots, - \beta(x^{\star})
\end{array}\right)^{\top}, b_O= \frac{1}{2} \beta(x^{\star}) + \int_{\partial \Omega} \beta(y) \frac{\partial \Phi(x^*, y)}{\partial n_y} d\sigma_y + \int_{\Omega} \Phi(x,y) \psi(y) dy,
$$

where $x^*:= (1, \phi) \in \partial \Omega$ is the unique point on the boundary corresponding to $x:= (r, \phi) \in \Omega$.  

Examples in Python can be seen [`here`](Scripts_and_Examples_Py/PFNN_Poisson_PDE.ipynb) and in MATLAB [`here`](Scripts_and_Examples/PFNN_Poisson_PDE_sparse_prediction_for_inverse.m).

The corresponding classes are in Python [`here`](Classes_Py/potential_fredholm_nn_models.py) and MATLAB [`here`](Classes/PotentialFredholmNeuralNetwork_Poisson.m).


### Helmholtz PDE - PFNN Construction 
The Helmholtz PDE:

$$
\begin{cases}
 \Delta u(x) - \lambda u(x) = \psi(x), \quad x \in \Omega \\ 
u(x) = f(x), \quad x \in \partial \Omega.   
\end{cases}
$$

can be solved using a Fredholm NN with $M+1$ hidden layers, where the first M layers solve the BIE on a discretized grid of the boundary, $y_1, \dots, y_N$. The final hidden and output layers are constructed according to the Fredholm NN representation of the double layer potential for PDE \eqref{helmholtz-pde}, with weights $W_{M+1} \in \mathbb{R}^{N \times N}, W_O \in \mathbb{R}^N$ given by:

$$
W_{M+1}= I_{N \times N},
\,\,\,\,\
W_{O}= \left(\begin{array}{cccc}
\mathcal{D} \Phi(x, y_1)\Delta \sigma_y, & \mathcal{D} \Phi(x, y_2)\Delta\sigma_y, & \dots, & \mathcal{D} \Phi(x, y_N) \Delta \sigma_y
\end{array}\right)^{\top},
$$

where, $\mathcal{D} \Phi(x, y_i)$ as defined in Proposition \ref{prop-poisson}. The corresponding biases $b_{M+1} \in \mathbb{R}^{N}$ and $b_O \in \mathbb{R}$ are given by:

$$
b_{M+1} = \left(\begin{array}{ccc}
-\beta(x^{\star}), \dots, - \beta(x^{\star})
\end{array}\right)^{\top}, b_O= \Big(\frac{1}{2} + \int_{\Omega} \lambda \delta \Phi(x, y) dy \Big) \beta(x^{\star}) + \int_{\partial \Omega} \beta(y) \frac{\partial \Phi(x^*, y)}{\partial n_y} d\sigma_y + \int_{\mathcal{D}} \Phi(x,y) f(y) dy,
$$

where we define $\delta\Phi(x,y) := \Phi(x,y) - \Phi(x^*,y)$.

For this case the fundamental solution is given by the modified Bessel function of the second kind $\Phi(x,y) = -\frac{1}{2 \pi} K_0(\lambda | x-y|).$

Examples in Python can be seen [`here`](Scripts_and_Examples_Py/PFNN_Helmholtz_PDE.ipynb) and in MATLAB [`here`](Scripts_and_Examples/PFNN_Helmholtz_PDE.m).

The corresponding classes are in Python [`here`](Classes_Py/potential_fredholm_nn_models.py) and MATLAB [`here`](Classes/PotentialFredholmNeuralNetwork_Helmholtz.m) and [`here`](Classes/PotentialFredholmNeuralNetwork_Helmholtz_dense.m).

### Semi-linear elliptic PDE - Recurrent PFNN Construction 
Consider the semi-linear PDE of the form: 

$$
\begin{cases}
\Delta u(x) = F(x, u(x)), \quad x \in \Omega \\
u(x) = f(x), \quad x \in \partial \Omega.
\end{cases} 
$$

For their solution, we employ a fixed point scheme which linearizes the PDE at each step of the iteration. In line with this approach, we consider the monotone iteration scheme , as below: 

1. Choose a $\lambda > 0$ "sufficiently large" (see below)
2. Take an initial guess $u_0(x)$
3. Solve, for $n = 0, 1, 2, \ldots$ the PDE:
4. Solve using the PFNN:
   
$$
\begin{cases}
\Delta u_{n+1}(x) - \lambda u_{n+1}(x) = -\lambda u_n(x) + F(x, u_n(x)), & x \in \Omega, \\
u_{n+1}(x) = f(x), & x \in \partial \Omega.
\end{cases}
$$

At each iteration we solve the PFNN for the Helmholtz PDE using the approximation for the integral with respect to the Poisson source at step n by:

$$
\int_{\Omega} \Phi(x, y) \psi_n(y) dy \approx \sum_{r \in \mathcal{R}} \sum_{\theta \in \Theta} \Phi(x,r, \theta) \psi_n(r,\theta)r \Delta r \Delta \theta.
$$   

Examples in Python can be seen [`here`](Scripts_and_Examples_Py/PFNN_Semi-linear_PDE.ipynb) and in MATLAB [`here`](Scripts_and_Examples/PFNN_Semi_linear_PDE.m).

The corresponding classes are in Python [`here`](Classes_Py/potential_fredholm_nn_models.py) and MATLAB [`here`](Classes/PotentialFredholmNeuralNetwork_Helmholtz.m).

### Application to the inverse source problem

We will be considering the inverse source problem for the PDE, consisting of a known boundary function $f : \partial \Omega \to {\mathbb R}$, as well as a coarse set of data points $\{u(x_i)\}_ {i=1,\cdots, n}, x_i \in \Omega $, and looking to approximate the unknown source function $\psi: \Omega \to {\mathbb R}$, using for a suitable model, represented by $\psi_ {\theta}: \Omega \to {\mathbb R}$ with parameters $\theta$ (e.g., a shallow neural network), such that the data $\{\tilde{u}(x_i) \}$ and function $f(x)$ satisfy the Poisson PDE. Our strategy takes advantage of the structure and convergence properties of the PFNN as follows: select a set of parameters $\theta$ such that when constructing the estimated kernel $\psi_{\theta}(\cdot, \cdot)$ and then feeding this into the PFNN with $M$ hidden layers, the output, $\hat{u}(x;\psi_{\theta})$ (which will also be denoted $\hat{u}(x)$ for brevity), is as ''close'' as possible (in terms of an appropriately chosen loss function) to the given data $\tilde{u}$. The learning problem then reduces to the optimization problem of tuning the parameters $\theta$ appropriately until we reach the optimal set $\theta^* $ and the corresponding source model $\psi_{\theta^*}$. 

The inverse problem is ill-posed. Hence, we require a regularization component. For our approach, where we use a shallow neural network model for the approximation $\psi_{\theta}$), we apply a Tikhonov regularization, encapsulated by the term:

$$
{R}(\theta) = \| \psi_\theta \|_2^2, = \sum_j (\hat{\psi}_{\theta}(x_j))^2, \,\,\ \text{for } x_j \in \Omega
$$

where and the complete loss function is given by: 

$$
L(\theta) = \frac{1}{N} \sum_{i=1}^{N} \Big(\tilde{u}(x_i) - \hat{u}(x_i; {\psi}_\theta) \Big)^2 + \lambda_{reg}{R}(\theta).
$$

<img width="622" height="245" alt="Screenshot 2025-10-08 at 5 04 35 PM" src="https://github.com/user-attachments/assets/c872ce08-1b5c-4ecc-a8ef-b7f9d78a9594" />

*Figure 5: Algorithm to solve inverse source problem for the Poisson PDE using the PFNN.*

Examples are in MATLAB [`here`](Scripts_and_Examples/Fredholm_Integral_Equation_inverse.m).

